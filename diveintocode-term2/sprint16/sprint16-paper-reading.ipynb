{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint16課題 論文読解入門"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## この課題の目的\n",
    "\n",
    "- 論文に触れ続ける一歩目を踏み出す\n",
    "- 論文から有益な情報を引き出せるようにする\n",
    "- これまで扱ってきた領域の論文から新たな知識を得る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.論文読解\n",
    "以下の論文を読み問題に答えてください。CNNを使った物体検出（Object Detection）の代表的な研究です。\n",
    "\n",
    "\\[8\\]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99\n",
    "\n",
    "[https://arxiv.org/pdf/1506.01497.pdf](https://arxiv.org/pdf/1506.01497.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題\n",
    "\n",
    "それぞれについてJupyter Notebookにマークダウン形式で記述してください。\n",
    "\n",
    "(1) 物体検出の分野にはどういった手法が存在したか。\n",
    "\n",
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "\n",
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "\n",
    "(4) RPNとは何か。\n",
    "\n",
    "(5) RoIプーリングとは何か。\n",
    "\n",
    "(6) Anchorのサイズはどうするのが適切か。\n",
    "\n",
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "\n",
    "(8) （アドバンス）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 条件\n",
    "\n",
    "- 答える際は論文のどの部分からそれが分かるかを書く。\n",
    "- 必要に応じて先行研究（引用されている論文）も探しにいく。最低2つは他の論文を利用して回答すること。\n",
    "- 論文の紹介記事を見ても良い。ただし、答えは論文内に根拠を探すこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 物体検出の分野にはどういった手法が存在したか。\n",
    "\n",
    "SPPnetやFast R-CNNなど\n",
    "\n",
    "**参照**\n",
    "- Abstract(P.1)  \n",
    ">Advances like SPPnet \\[1\\] and Fast R-CNN \\[2\\] have reduced the running time of these detection networks, exposing region\n",
    "proposal computation as a bottleneck. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "\n",
    "Faster R-CNNは、Fast R-CNNのSelective Searchの代わりにRPNを用いており、end-to-endで学習をすることで高速化を実現している。\n",
    "\n",
    "\n",
    "**参照**\n",
    "\n",
    "- Abstract(P.1)\n",
    ">In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with “attention” mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model \\[3\\], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "\n",
    "- One-stage  \n",
    "畳み込み特徴マップ上で回帰と分類を行う手法\n",
    "\n",
    "- Two-Stage  \n",
    "領域候補を提案する畳み込みネットワークと、提案された領域候補を使用するFast R-CNN検出器の2つのモジュールからなる手法\n",
    "\n",
    "**参照**\n",
    "\n",
    "- 3 FASTER R-CNN(P.3)  \n",
    ">Our object detection system, called Faster R-CNN, is composed of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector \\[2\\] that uses the proposed regions. The entire system is a single, unified network for object detection (Figure 2).\n",
    "\n",
    "- 4 EXPERIMENTS  \n",
    "4.1Experiments on PASCAL VOC(P.10)  \n",
    ">One-Stage Detection vs. Two-Stage Proposal + Detection. The OverFeat paper \\[9\\] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. OverFeat is a one-stage, class-specific detection pipeline, and ours is a two-stage cascade consisting of class-agnostic proposals and class-specific detections. In OverFeat, the region-wise features come from a sliding window of one aspect ratio over a scale pyramid. These features are used to simultaneously determine the location and category of objects. In RPN, the features are from square (3 $\\times$ 3) sliding windows and predict proposals relative to anchors with different scales and aspect ratios. Though both methods use sliding windows, the region proposal task is only the first stage of Faster RCNN—the downstream Fast R-CNN detector attends to the proposals to refine them. In the second stage of our cascade, the region-wise features are adaptively pooled \\[1\\], \\[2\\] from proposal boxes that more faithfully cover the features of the regions. We believe these features lead to more accurate detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) RPNとは何か。\n",
    "\n",
    "畳み込み特徴マップ上をスライドし、それぞれが物体かどうかを表すスコアと、長方形の物体の領域候補を出力するネットワーク。\n",
    "\n",
    "**参照**\n",
    "\n",
    "- 3 FASTER R-CNN  \n",
    "3.1 Region Proposal Networks(P.3)  \n",
    ">A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score. We model this process with a fully convolutional network \\[7\\], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN object detection network \\[2\\], we assume that both nets share a common set of convolutional layers. In our experiments, we investigate the Zeiler and Fergus model \\[32\\] (ZF), which has 5 shareable convolutional layers and the Simonyan and Zisserman model \\[3\\] (VGG-16), which has 13 shareable convolutional layers. To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer. This small network takes as input an n × n spatial window of the input convolutional feature map. Each sliding window is mapped to a lower-dimensional feature (256-d for ZF and 512-d for VGG, with ReLU \\[33\\] following). This feature is fed into two sibling fullyconnected layers—a box-regression layer (reg) and a box classification layer (cls). We use n = 3 in this paper, noting that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-network is illustrated at a single position in Figure 3 (left). Note that because the mini-network operates in a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This architecture is naturally implemented with an n×n convolutional layer followed by two sibling 1 $\\times$ 1 convolutional layers (for reg and cls, respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) RoIプーリングとは何か。\n",
    "\n",
    "畳み込みを行なった特徴マップから、Maxプーリングを使用してハイパーパラメータで固定されたサイズの小さな特徴マップに変換すること。\n",
    "\n",
    "**参照**\n",
    "\n",
    "- [Fast R-CNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf)の論文内より  \n",
    "2.Fast R-CNN architecture and training  \n",
    "2.1. The RoI pooling layer(P1441)  \n",
    "\n",
    "\n",
    ">The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H $\\times$ W (e.g., 7 $\\times$ 7), where H and W are layer hyper-parameters that are independent of any particular RoI. In this paper, an RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple (r, c, h, w) that specifies its top-left corner (r, c) and its height and width (h, w).\n",
    "RoI max pooling works by dividing the h × w RoI window into an H × W grid of sub-windows of approximate size h/H × w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling. The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets \\[11\\] in which there is only one pyramid level. We use the pooling sub-window calculation given in \\[11\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Anchorのサイズはどうするのが適切か。\n",
    "\n",
    "論文内のデフォルトではアンカーのサイズは、スケールが128^2, 256^2, 512^2の3種類と、アスペクト比が1:1, 1:2, 2:1の3種類の合計9つとなっているが、入力する画像に合わせることが適切である。\n",
    "\n",
    "**参照**\n",
    "\n",
    "- 3 FASTER R-CNN  \n",
    "3.3 Implementation Details(P.6)\n",
    "\n",
    ">For anchors, we use 3 scales with box areas of 1282, 2562, and 5122 pixels, and 3 aspect ratios of 1:1, 1:2, and 2:1. These hyper-parameters are not carefully chosen for a particular dataset, and we provide ablation experiments on their effects in the next section. As discussed, our solution does not need an image pyramid or filter pyramid to predict regions of multiple scales, saving considerable running time. Figure 3 (right) shows the capability of our method for a wide range of scales and aspect ratios. Table 1 shows the learned average proposal size for each anchor using the ZF net. We note that our algorithm allows predictions that are larger than the underlying receptive field. Such predictions are not impossible—one may still roughly infer the extent of an object if only the middle of the object is visible.  \n",
    "The anchor boxes that cross image boundaries need to be handled with care. During training, we ignore all cross-boundary anchors so they do not contribute to the loss. For a typical 1000 × 600 image, there will be roughly 20000 (≈ 60 × 40 × 9) anchors in total. With the cross-boundary anchors ignored, there are about 6000 anchors per image for training. If the boundary-crossing outliers are not ignored in training, they introduce large, difficult to correct error terms in the objective, and training does not converge. During testing, however, we still apply the fully convolutional RPN to the entire image. This may generate crossboundary proposal boxes, which we clip to the image boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "\n",
    "データセットとしてPASCAL VOC(2007), PASCAL VOC(2012), Microsoft COCOを用いている。\n",
    "\n",
    "- 学習データにPASCAL VOC(2007)のtrainvalとPASCAL VOC(2012)のtrainvalを、テストデータとしてPASCAL VOC(2007)を用いた時、FAST R-CNNにRPN+VGGを組み合わせたときのmAPは73.2%であり、従来のSelective Search(SS)を用いた時の70.0%より上昇した。\n",
    "\n",
    "\n",
    "- 学習データにPASCAL VOC(2007)のtrainval+testとPASCAL VOC(2012)のtrainvalを、テストデータとしてPASCAL VOC(2012)を用いた時、FAST R-CNNにRPN+VGGを組み合わせたときのmAPは70.4%であり、従来のSelective Search(SS)を用いた時の68.4%より上昇した。\n",
    "\n",
    "\n",
    "- VGGにSS + Fast R-CNNを組み合わせた時の処理時間が1830msだったものが、RPN + Fast R-CNNを組み合わせたものでは198msと約1/10に短縮された。\n",
    "\n",
    "\n",
    "- 学習データにMS COCO train、テストデータにMS COCO test、モデルにVGG16を用いた時、Faster R-CNNではmAP\\@0.5で42.1%、mAP\\@\\[.5, .95\\]で21.5%と、FAST R-CNNのmAP\\@0.5%の39.3%、\\[.5, .95\\]の19.3%より高くなった。また、学習データがMS COCO trainvalデータセットの時は、Faster R-CNNmAP\\@0.5で42.7%、mAP\\@\\[.5, .95\\]で21.9%となった。\n",
    "\n",
    "**参照**\n",
    "\n",
    "- 4 EXPERIMENTS  \n",
    "4.1 Experiments on PASCAL VOC\n",
    "\n",
    ">(P.8)  \n",
    "Table 3 shows the results of VGG-16 for both proposal and detection. Using RPN+VGG, the result is 68.5% for unshared features, slightly higher than the SS baseline. As shown above, this is because the proposals generated by RPN+VGG are more accurate than SS. Unlike SS that is predefined, the RPN is actively trained and benefits from better networks. For the feature-shared variant, the result is 69.9%—better than the strong SS baseline, yet with nearly cost-free proposals. We further train the RPN and detection network on the union set of PASCAL VOC 2007 trainval and 2012 trainval. The mAP is 73.2%. Figure 5 shows some results on the PASCAL VOC 2007 test set. On the PASCAL VOC 2012 test set (Table 4), our method has an mAP of 70.4% trained on the union set of VOC 2007 trainval+test and VOC 2012 trainval. Table 6 and Table 7 show the detailed numbers.\n",
    "\n",
    ">(P.8)  \n",
    "In Table 5 we summarize the running time of the entire object detection system. SS takes 1-2 seconds depending on content (on average about 1.5s), and Fast R-CNN with VGG-16 takes 320ms on 2000 SS proposals (or 223ms if using SVD on fully-connected layers \\[2\\]). Our system with VGG-16 takes in total 198ms for both proposal and detection. With the convolutional features shared, the RPN alone only takes 10ms computing the additional layers. Our regionwise computation is also lower, thanks to fewer proposals (300 per image). Our system has a frame-rate of 17 fps with the ZF net.\n",
    "\n",
    ">(P.11)  \n",
    "In Table 11 we first report the results of the Fast R-CNN system \\[2\\] using the implementation in this paper. Our Fast R-CNN baseline has 39.3% mAP\\@0.5 on the test-dev set, higher than that reported in \\[2\\]. We conjecture that the reason for this gap is mainly due to the definition of the negative samples and also the changes of the mini-batch sizes. We also note that the mAP\\[.5, .95\\] is just comparable.   \n",
    "Next we evaluate our Faster R-CNN system. Using the COCO training set to train, Faster R-CNN has 42.1% mAP\\@0.5 and 21.5% mAP\\[.5, .95\\] on the COCO test-dev set. This is 2.8% higher for mAP\\@0.5 and 2.2% higher for mAP\\@\\[.5, .95\\] than the Fast RCNN counterpart under the same protocol (Table 11). This indicates that RPN performs excellent for improving the localization accuracy at higher IoU thresholds. Using the COCO trainval set to train, Faster RCNN has 42.7% mAP\\@0.5 and 21.9% mAP\\@\\[.5, .95\\] on the COCO test-dev set. Figure 6 shows some results on the MS COCO test-dev set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
