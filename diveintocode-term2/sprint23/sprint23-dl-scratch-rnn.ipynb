{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint23課題 深層学習スクラッチリカレントニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## この課題の目的\n",
    "\n",
    "- スクラッチを通してRNNの基礎を理解する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スクラッチによる実装\n",
    "\n",
    "NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "Sprint11で作成したディープニューラルネットワークのクラスを拡張する形でRNNを作成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
    "\n",
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
    "\n",
    "今回はバッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。\n",
    "\n",
    "$$a_t = x_{t}\\cdot W_{x} + h_{t-1}\\cdot W_{h} + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_t = tanh(a_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a_t$ : 時刻tの活性化関数を通す前の状態 (batch_size, n_nodes)\n",
    "\n",
    "$h_t$ : 時刻tの状態・出力 (batch_size, n_nodes)\n",
    "\n",
    "$x_t$ : 時刻tの入力 (batch_size, n_features)\n",
    "\n",
    "$W_x$ : 入力に対する重み (n_features, n_nodes)\n",
    "\n",
    "$h_{t-1}$ : 時刻t-1の状態（前の時刻から伝わる順伝播） (batch_size, n_nodes)\n",
    "\n",
    "$W_h$ : 状態に対する重み。 (n_nodes, n_nodes)\n",
    "\n",
    "$b$ : バイアス項 (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期状態 $h_0$ は全て0とすることが多いですが、任意の値を与えることも可能です。\n",
    "\n",
    "上記の処理を系列数n_sequences回繰り返すことになります。RNN全体への入力 $x$ は(batch_size, n_sequences, n_features)のような配列で渡されることになり、そこから各時刻の配列を取り出していきます。\n",
    "\n",
    "分類問題であれば、それぞれの時刻の$h$に対して全結合層とソフトマックス関数（またはシグモイド関数）を使用します。出力は最後の$h$だけを使用する場合と、全ての$h$を使う場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    シンプルなRNN\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes : int\n",
    "      ノード数\n",
    "    n_features : int\n",
    "      特徴量数\n",
    "    initializer : インスタンス\n",
    "        初期化方法のインスタンス\n",
    "    optimizer : インスタンス\n",
    "        最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes, n_features, initializer, optimizer):\n",
    "        self.optimizer = optimizer #最適化手法\n",
    "        \n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.Wx = initializer.W(n_features, n_nodes)\n",
    "        self.Wh = initializer.W(n_nodes, n_nodes)\n",
    "        self.B = initializer(1,)\n",
    "\n",
    "        self.h = None\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワードプロバケーション\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_sequences, n_features)\n",
    "            入力\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        self.h : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        #サイズの取得\n",
    "        batch_size = X.shape[0] #バッチサイズ\n",
    "        n_sequences = X.shape[1] #シーケンス数\n",
    "        \n",
    "        #hの初期値\n",
    "        self.h = np.zeros([batch_size, n_nodes])\n",
    "\n",
    "        #hを算出\n",
    "        for i in range(n_sequences):\n",
    "            X_t = np.dot(X[:, i, :], self.Wx) + np.dot(self.h, self.Wh) + self.B\n",
    "            self.h = np.tanh(X_t)\n",
    "        \n",
    "        return self.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
    "\n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "\n",
    "入力$x$、初期状態$h$、重み$w\\_x$と$w\\_h$、バイアス$b$を次のようにします。\n",
    "\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes))\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォワードプロパゲーションの出力が次のようになることを作成したコードで確認してください。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重みとバイアスは初期設定書き換え\n",
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    シンプルなRNN\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : インスタンス\n",
    "        初期化方法のインスタンス\n",
    "    optimizer : インスタンス\n",
    "        最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes, n_features, initializer, optimizer):\n",
    "        self.optimizer = optimizer #最適化手法\n",
    "        \n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        #self.Wx = initializer.W(n_features, n_nodes)\n",
    "        #self.Wh = initializer.W(n_nodes, n_nodes)\n",
    "        #self.B = initializer(1,)\n",
    "        \n",
    "        #重みとバイアスの初期設定値\n",
    "        self.Wx= np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100\n",
    "        self.Wh = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100\n",
    "        self.B = np.array([1])\n",
    "        \n",
    "        self.h = None\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワードプロバケーション\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_sequences, n_features)\n",
    "            入力\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        self.h : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        #サイズの取得\n",
    "        batch_size = X.shape[0] #バッチサイズ\n",
    "        n_sequences = X.shape[1] #シーケンス数\n",
    "        \n",
    "        #hの初期値\n",
    "        self.h = np.zeros([batch_size, n_nodes])\n",
    "\n",
    "        #hを算出\n",
    "        for i in range(n_sequences):\n",
    "            X_t = np.dot(X[:, i, :], self.Wx) + np.dot(self.h, self.Wh) + self.B\n",
    "            self.h = np.tanh(X_t)\n",
    "        \n",
    "        return self.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#フォワードの実行\n",
    "SimpleRNN(n_nodes, _, _, _).forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】（アドバンス課題）バックプロパゲーションの実装\n",
    "\n",
    "バックプロパゲーションを実装します。\n",
    "\n",
    "RNNの内部は全結合層を組み合わせた形になっているので、更新式は全結合層などと同様です。\n",
    "\n",
    "$$W_x^{\\prime} = W_x - \\alpha E(\\frac{\\partial L}{\\partial W_x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W_h^{\\prime} = W_h - \\alpha E(\\frac{\\partial L}{\\partial W_h})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$b^{\\prime} = b - \\alpha E(\\frac{\\partial L}{\\partial b})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ : 学習率\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_x}$ : $W_x$ に関する損失 $L$ の勾配\n",
    " \n",
    "$\\frac{\\partial L}{\\partial W_h}$ : $W_h$ に関する損失 $L$ の勾配\n",
    " \n",
    "$\\frac{\\partial L}{\\partial b}$ : $b$ に関する損失 $L$ の勾配\n",
    " \n",
    "$E()$ : ミニバッチ方向にベクトルの平均を計算\n",
    "\n",
    "勾配を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "$\\frac{\\partial h_t}{\\partial a_t} = \\frac{\\partial L}{\\partial h_t} \\times(1-tanh^2(a_t))$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = \\frac{\\partial h_t}{\\partial a_t}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_x} = x_{t}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_h} = h_{t-1}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}$\n",
    "\n",
    "＊$\\frac{\\partial L}{\\partial h_t}$ は前の時刻からの状態の誤差と出力の誤差の合計です。hは順伝播時に出力と次の層に伝わる状態双方に使われているからです。\n",
    "\n",
    "前の時刻や層に流す誤差の数式は以下です。\n",
    "\n",
    "$\\frac{\\partial L}{\\partial h_{t-1}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{h}^{T}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_{t}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{x}^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】（アドバンス課題）データセットでの学習・推定\n",
    "\n",
    "これまで使ってきたニューラルネットワークにSimpleRNNを組み込み学習させ、動くことを確認してください。\n",
    "\n",
    "[IMDB Review Dataset | Kaggle](https://www.kaggle.com/utathya/imdb-review-dataset)\n",
    "\n",
    "映画レビューデータセットを使用します。ベクトル化を行い、作成したRNNに入力してください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
