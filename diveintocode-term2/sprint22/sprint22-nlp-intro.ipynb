{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint22課題 自然言語処理入門"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## この課題の目的\n",
    "\n",
    "- 自然言語処理を体験する\n",
    "- 簡単な文書の分析ができるようになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.charfilter import *\n",
    "from janome.tokenfilter import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, GRU, Embedding, Input\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from tensorflow.python.keras import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】BoWとN-gram(手計算)\n",
    "\n",
    "目的\n",
    "\n",
    "- 古典的かつ強力な手法BoWとN-gramの理解\n",
    "\n",
    "以下は俳優K.Kさんのつぶやき(コーパス)です。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "文1: 今撮影中で〜す！  \n",
    "文2: 今休憩中で〜す(^^)  \n",
    "文3: 今日ドラマ撮影で〜す！  \n",
    "文4: 今日、映画瞬公開で〜す！！！  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問】  \n",
    "特殊文字除去(!や〜など)、単語分割をし以下の2パターンで文1〜文4を数値化(ベクトル化)してください。\n",
    "\n",
    "- BoW(1-gram)  \n",
    "- BoW(2-gram)  \n",
    "手計算の後見やすい形にしてください。\n",
    "\n",
    "例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>love</th>\n",
       "      <th>this</th>\n",
       "      <th>is</th>\n",
       "      <th>the</th>\n",
       "      <th>baseball</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I love baseball !!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I love this !</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    I  love  this  is  the  baseball\n",
       "I love baseball !!  1     1     0   0    0         1\n",
       "I love this !       1     1     1   0    0         0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = [\"I\", \"love\", \"this\", \"is\", \"the\", \"baseball\"]\n",
    "ms_kk_texts = [\"I love baseball !!\", \"I love this !\"]\n",
    "texts_vec = [[1,1,0,0,0,1], [1,1,1,0,0,0]]\n",
    "\n",
    "df_bow_1gram = pd.DataFrame(data = texts_vec, columns = vocabulary, index = ms_kk_texts)\n",
    "df_bow_1gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW(1-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>今</th>\n",
       "      <th>今日</th>\n",
       "      <th>ドラマ</th>\n",
       "      <th>映画</th>\n",
       "      <th>瞬</th>\n",
       "      <th>撮影</th>\n",
       "      <th>休憩</th>\n",
       "      <th>中</th>\n",
       "      <th>公開</th>\n",
       "      <th>です</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>今撮影中で〜す！</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今休憩中で〜す(^^)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今日ドラマ撮影で〜す！</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今日、映画瞬公開で〜す！！！</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                今  今日  ドラマ  映画  瞬  撮影  休憩  中  公開  です\n",
       "今撮影中で〜す！        1   0    0   0  0   1   0  1   0   1\n",
       "今休憩中で〜す(^^)     1   0    0   0  0   0   1  1   0   1\n",
       "今日ドラマ撮影で〜す！     0   1    1   0  0   1   0  0   0   1\n",
       "今日、映画瞬公開で〜す！！！  0   1    0   1  1   0   0  0   1   1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#特殊文字除去後の単語\n",
    "vocabulary = ['今', '今日', 'ドラマ', '映画', '瞬', '撮影', '休憩', '中', '公開', 'です']\n",
    "\n",
    "#文章\n",
    "kk_texts = ['今撮影中で〜す！', \n",
    "                    '今休憩中で〜す(^^)', \n",
    "                    '今日ドラマ撮影で〜す！', \n",
    "                    '今日、映画瞬公開で〜す！！！']\n",
    "\n",
    "texts_vec = [[1, 0, 0, 0, 0, 1, 0, 1, 0, 1], \n",
    "                        [1, 0, 0, 0, 0, 0, 1, 1, 0, 1],\n",
    "                        [0, 1, 1, 0, 0, 1, 0, 0, 0, 1],\n",
    "                        [0, 1, 0, 1, 1, 0, 0, 0, 1, 1]]\n",
    "\n",
    "df_bow_1gram = pd.DataFrame(data = texts_vec, columns = vocabulary, index = kk_texts)\n",
    "df_bow_1gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW(2-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>今撮影</th>\n",
       "      <th>撮影中</th>\n",
       "      <th>中です</th>\n",
       "      <th>今休憩</th>\n",
       "      <th>休憩中</th>\n",
       "      <th>今日ドラマ</th>\n",
       "      <th>ドラマ撮影</th>\n",
       "      <th>撮影です</th>\n",
       "      <th>今日映画</th>\n",
       "      <th>映画瞬</th>\n",
       "      <th>瞬公開</th>\n",
       "      <th>公開です</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>今撮影中で〜す！</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今休憩中で〜す(^^)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今日ドラマ撮影で〜す！</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今日、映画瞬公開で〜す！！！</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                今撮影  撮影中  中です  今休憩  休憩中  今日ドラマ  ドラマ撮影  撮影です  今日映画  映画瞬  瞬公開  \\\n",
       "今撮影中で〜す！          1    1    1    0    0      0      0     0     0    0    0   \n",
       "今休憩中で〜す(^^)       0    0    1    1    1      0      0     0     0    0    0   \n",
       "今日ドラマ撮影で〜す！       0    0    0    0    0      1      1     1     0    0    0   \n",
       "今日、映画瞬公開で〜す！！！    0    0    0    0    0      0      0     0     1    1    1   \n",
       "\n",
       "                公開です  \n",
       "今撮影中で〜す！           0  \n",
       "今休憩中で〜す(^^)        0  \n",
       "今日ドラマ撮影で〜す！        0  \n",
       "今日、映画瞬公開で〜す！！！     1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#特殊文字除去後の単語\n",
    "vocabulary = ['今撮影', '撮影中', '中です', '今休憩', '休憩中', '今日ドラマ', 'ドラマ撮影', \n",
    "                          '撮影です', '今日映画', '映画瞬', '瞬公開', '公開です']\n",
    "\n",
    "#文章\n",
    "kk_texts = ['今撮影中で〜す！', \n",
    "                    '今休憩中で〜す(^^)', \n",
    "                    '今日ドラマ撮影で〜す！', \n",
    "                    '今日、映画瞬公開で〜す！！！']\n",
    "\n",
    "texts_vec = [[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "                        [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                        [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n",
    "                        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]]\n",
    "\n",
    "df_bow_1gram = pd.DataFrame(data = texts_vec, columns = vocabulary, index = kk_texts)\n",
    "df_bow_1gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】TF-IDF(手計算)\n",
    "\n",
    "目的\n",
    "\n",
    "- 古典的かつ強力なTF-IDFの理解\n",
    "標準的なTF-IDFの公式 \n",
    "\n",
    "Term Frequency:\n",
    "\n",
    "$$tf(t,d) = \\frac{n_{t,d}}{\\sum_{s \\in d}n_{s,d}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n_{t,d}$ : 文書d内の単語tの出現回数\n",
    "\n",
    "$\\sum_{s \\in d}n_{s,d}$ : 文書dの全単語の出現回数の和\n",
    "\n",
    "Inverse Document Frequency:\n",
    "\n",
    "$$idf(t) = \\log_2{\\frac{N}{df(t)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N$ : 全文書数\n",
    "\n",
    "$df(t)$ : 単語tが出現する文書数\n",
    "\n",
    "TF-IDF:\n",
    "\n",
    "$$tfidf(t, d) = tf(t, d) \\times idf(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問】\n",
    "問題1のコーパスを使って、文1〜文4をTFIDFで数値化(ベクトル化)してください。   \n",
    "問題1と同様、手計算の後見やすい形にしてください。\n",
    "\n",
    "正解例  \n",
    "tfidf(今, 文書1) = 0.25 になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(t_num, t_sum, n, df):\n",
    "    '''\n",
    "    TF-IDFの計算\n",
    "    \n",
    "    Parameters\n",
    "    t_num : int\n",
    "        文書d内の単語tの出現回数\n",
    "    t_sum : int\n",
    "        文書dの全単語の出現回数の和\n",
    "    n : int\n",
    "         全文書数\n",
    "    df : int\n",
    "        単語tが出現する文書数\n",
    "    '''\n",
    "    tf = t_num / t_sum\n",
    "    \n",
    "    idf = np.log2(n / df)\n",
    "    \n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特殊文字除去後の単語\n",
    "vocabulary = ['今', '今日', 'ドラマ', '映画', '瞬', '撮影', '休憩', '中', '公開', 'です']\n",
    "\n",
    "#文章\n",
    "kk_texts = ['今撮影中で〜す！', \n",
    "                    '今休憩中で〜す(^^)', \n",
    "                    '今日ドラマ撮影で〜す！', \n",
    "                    '今日、映画瞬公開で〜す！！！']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idfを求める(単語_文章番号)\n",
    "#文章1\n",
    "ima_1 = tf_idf(1, 4, 4, 2)\n",
    "satuei_1 = tf_idf(1, 4, 4, 2)\n",
    "tyuu_1 = tf_idf(1, 4, 4, 2)\n",
    "desu_1 = tf_idf(1, 4, 4, 4)\n",
    "\n",
    "#文章2\n",
    "ima_2 = tf_idf(1, 4, 4, 2)\n",
    "kyuukei_2 = tf_idf(1, 4, 4, 1)\n",
    "tyuu_2 = tf_idf(1, 4, 4, 2)\n",
    "desu_2 = tf_idf(1, 4, 4, 4)\n",
    "\n",
    "#文章3\n",
    "kyou_3 = tf_idf(1, 4, 4, 2)\n",
    "drama_3 = tf_idf(1, 4, 4, 2)\n",
    "satuei_3 = tf_idf(1 ,4, 4, 2)\n",
    "desu_3 = tf_idf(1, 4, 4, 4)\n",
    "\n",
    "#文章4\n",
    "kyou_4 = tf_idf(1, 5, 4, 2)\n",
    "eiga_4 = tf_idf(1, 5, 4, 1)\n",
    "matataki_4 = tf_idf(1, 5, 4, 1)\n",
    "koukai_4 = tf_idf(1, 5, 4, 1)\n",
    "desu_4 = tf_idf(1, 5, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>今</th>\n",
       "      <th>今日</th>\n",
       "      <th>ドラマ</th>\n",
       "      <th>映画</th>\n",
       "      <th>瞬</th>\n",
       "      <th>撮影</th>\n",
       "      <th>休憩</th>\n",
       "      <th>中</th>\n",
       "      <th>公開</th>\n",
       "      <th>です</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>今撮影中で〜す！</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今休憩中で〜す(^^)</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今日ドラマ撮影で〜す！</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今日、映画瞬公開で〜す！！！</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   今    今日   ドラマ   映画    瞬    撮影   休憩     中   公開   です\n",
       "今撮影中で〜す！        0.25  0.00  0.00  0.0  0.0  0.25  0.0  0.25  0.0  0.0\n",
       "今休憩中で〜す(^^)     0.25  0.00  0.00  0.0  0.0  0.00  0.5  0.25  0.0  0.0\n",
       "今日ドラマ撮影で〜す！     0.00  0.25  0.25  0.0  0.0  0.25  0.0  0.00  0.0  0.0\n",
       "今日、映画瞬公開で〜す！！！  0.00  0.20  0.00  0.4  0.4  0.00  0.0  0.00  0.4  0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_vec = [[ima_1, 0, 0, 0, 0, satuei_1, 0, tyuu_1, 0, desu_1], \n",
    "                        [ima_2, 0, 0, 0, 0, 0, kyuukei_2, tyuu_2, 0, desu_2],\n",
    "                        [0, kyou_3, drama_3, 0, 0, satuei_3, 0, 0, 0, desu_3],\n",
    "                        [0, kyou_4, 0, eiga_4, matataki_4, 0, 0, 0, koukai_4, desu_4]]\n",
    "\n",
    "df_bow_tfidf = pd.DataFrame(data = texts_vec, columns = vocabulary, index = kk_texts)\n",
    "df_bow_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】テキストクリーニング(プログラミング)\n",
    "目的\n",
    "\n",
    "- 実データ対応のためのテキストクリーニングの理解\n",
    "- 正規表現の理解\n",
    "\n",
    "実際のテキストデータは非常に汚いことが多いです。   \n",
    "以下は3/6(水)にnoroさんがSlackで発言した文章で、良い例です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<!everyone> *【スペシャル特典】有償のRubyMineやPyCharmの `6ヶ月間100%OFFクーポン` をご希望者の方先着100名様に贈呈いたします！*\\n\\nこの度、RubyMineやPyCharmのメーカーであるJetBrains社へのクーポンコードの提供交渉が実り、100クーポンをいただくことができました。\\n\\n```\\nRubyMine\\n<https://www.jetbrains.com/ruby/>\\n\\nPyCharm\\n<https://www.jetbrains.com/pycharm/>\\n```\\n\\n「ご希望の方は、手を挙げて！」方式で、ご希望の方はこの投稿の手あげスタンプをクリックしてください。\\n\\n期限は、 *`2019年3月20日（水）22:00まで`* とさせていただきます。\\nふるってのご希望をお待ちしております！ :smile:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問】\n",
    "このテキストに以下の処理を施してください。\n",
    "\n",
    "- urlを削除\n",
    "- 【〇〇】を削除\n",
    "- 改行等の特殊文字を削除\n",
    "- 絵文字除去\n",
    "\n",
    "ここではしませんが、数字を文字列NUMBERに置き換える処理をよくします。\n",
    "\n",
    "正解例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "有償のRubyMineやPyCharmの6ヶ月間100%OFFクーポンをご希望者の方先着100名様に贈呈いたします！この度、RubyMineやPyCharmのメーカーであるJetBrains社へのクーポンコードの提供交渉が実り、100クーポンをいただくことができました。RubyMinePyCharm「ご希望の方は、手を挙げて！」方式で、ご希望の方はこの投稿の手あげスタンプをクリックしてください。期限は、2019年3月20日（水）22:00までとさせていただきます。ふるってのご希望をお待ちしております！\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正規表現のサンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'【スペシャル特典】有償のRubyMineやPyCharmの 6ヶ月間100%OFFクーポン をご希望者の方先着100名様に贈呈いたします'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 対象テキストデータ\n",
    "text = '【スペシャル特典】有償のRubyMineやPyCharmの `6ヶ月間100%OFFクーポン` をご希望者の方先着100名様に贈呈いたします！*\\n\\n'\n",
    "# re.compileを使うと処理が早くなります\n",
    "BAD_SYMBOL = re.compile('[\\n*！`]+')\n",
    "# re.sub(r'[\\n*！`]+', '', text)でもできます\n",
    "text = re.sub(BAD_SYMBOL, '', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '<!everyone> *【スペシャル特典】有償のRubyMineやPyCharmの `6ヶ月間100%OFF'\\\n",
    "            'クーポン` をご希望者の方先着100名様に贈呈いたします！*\\n\\nこの度、RubyMineや'\\\n",
    "            'PyCharmのメーカーであるJetBrains社へのクーポンコードの提供交渉が実り、100クーポン'\\\n",
    "        'をいただくことができました。\\n\\n```\\nRubyMine\\n<https://www.jetbrains.com/ruby/>'\\\n",
    "        '\\n\\nPyCharm\\n<https://www.jetbrains.com/pycharm/>\\n```\\n\\n「ご希望の方は、'\\\n",
    "        '手を挙げて！」方式で、ご希望の方はこの投稿の手あげスタンプをクリックしてください。\\n\\n期限は、'\\\n",
    "        '*`2019年3月20日（水）22:00まで`* とさせていただきます。\\nふるってのご希望をお待ちしております！ :smile:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<>\n",
    "url = '<.*?>'\n",
    "\n",
    "#【】\n",
    "kakko = '【.*?】'\n",
    "\n",
    "#特殊文字\n",
    "tokusyu = '[\\n\\*`\\s]'\n",
    "\n",
    "#絵文字\n",
    "emoji = ':[^0-90-9]+:'\n",
    "\n",
    "#連結\n",
    "reg_str = f'{url}|{kakko}|{tokusyu}|{emoji}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有償のRubyMineやPyCharmの6ヶ月間100%OFFクーポンをご希望者の方先着100名様に贈呈いたします！この度、RubyMineやPyCharmのメーカーであるJetBrains社へのクーポンコードの提供交渉が実り、100クーポンをいただくことができました。RubyMinePyCharm「ご希望の方は、手を挙げて！」方式で、ご希望の方はこの投稿の手あげスタンプをクリックしてください。期限は、2019年3月20日（水）22:00までとさせていただきます。ふるってのご希望をお待ちしております！\n"
     ]
    }
   ],
   "source": [
    "#削除\n",
    "remove = re.compile(reg_str)\n",
    "text = re.sub(remove, '', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "リアルタイムで正規表現を確認できるサイトです。\n",
    "\n",
    "[https://regex101.com/](https://regex101.com/)\n",
    "\n",
    "[https://regex-testdrive.com/ja/dotest](https://regex-testdrive.com/ja/)\n",
    "\n",
    "[re 正規表現操作](https://docs.python.org/ja/3/library/re.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips NLPのLinuxコマンド\n",
    "\n",
    "これまでpythonでファイルを読み込んで処理をしていましたが、  \n",
    "簡単な作業においてはlinuxコマンドの方がメモリの使用料が半分以下だったりとパフォーマンスが良いです。\n",
    "\n",
    "例えばファイルの行数を数えたい場合、pythonでわざわざ書くのは面倒です。  \n",
    "以下の1行のコマンドで実行できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "wc -l 〇〇.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また分割したい場合はsplit  \n",
    "並び替えたい場合はsort  \n",
    "置換にはsed  \n",
    "文の先頭、後頭部分を見たければhead,tail  \n",
    "など便利なコマンドがあります。  \n",
    "詳しく知りたい方はNLP100本ノックで調べてみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】形態素解析\n",
    "\n",
    "目的\n",
    "\n",
    "- 形態素解析の理解\n",
    "\n",
    "形態素解析のツールはMecabやJanomeなど様々ですが、  \n",
    "ここでは手軽に導入できるJanomeを使います。\n",
    "\n",
    "[Janome document](https://mocobeta.github.io/janome/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問】\n",
    "\n",
    "上記のクリーニングしたテキストをJanomeを用いて形態素解析をし、  \n",
    "名詞または動詞の単語を抜き出してください。\n",
    "\n",
    "正解例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[\"有償\", \"RubyMine\", \"Pycharm\", ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['有償', 'RubyMine', 'PyCharm', '6', 'ヶ月', '間', '100', '%', 'OFF', 'クーポン', '希望', '者', '方', '先着', '100', '名', '様', '贈呈', 'いたし', '度', 'RubyMine', 'PyCharm', 'メーカー', 'JetBrains', '社', 'クーポン', 'コード', '提供', '交渉', '実り', '100', 'クーポン', 'いただく', 'こと', 'でき', 'RubyMinePyCharm', '希望', '方', '手', '挙げ', '方式', '希望', '方', '投稿', '手', 'あげ', 'スタンプ', 'クリック', 'し', 'ください', '期限', '2019', '年', '3', '月', '20', '日', '水', '22', ':', '00', 'さ', 'せ', 'いただき', '希望', 'お待ち', 'し', 'おり']\n"
     ]
    }
   ],
   "source": [
    "#動詞or名詞なら抜き出す\n",
    "t = Tokenizer()\n",
    "print([token.surface for token in t.tokenize(text) if token.part_of_speech.split(',')[0] in ['動詞', '名詞']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**複合名詞化の場合**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['有償', 'RubyMine', 'PyCharm', '6ヶ月間100%OFFクーポン', '希望者', '方先着100名様', '贈呈', 'いたし', '度', 'RubyMine', 'PyCharm', 'メーカー', 'JetBrains社', 'クーポンコード', '提供交渉', '実り', '100クーポン', 'いただく', 'こと', 'でき', 'RubyMinePyCharm', '希望', '方', '手', '挙げ', '方式', '希望', '方', '投稿', '手', 'あげ', 'スタンプ', 'クリック', 'し', 'ください', '期限', '2019年3月20日', '水', '22:00', 'さ', 'せ', 'いただき', '希望', 'お待ち', 'し', 'おり']\n"
     ]
    }
   ],
   "source": [
    "#動詞or名詞なら抜き出す\n",
    "a = Analyzer(token_filters=[CompoundNounFilter()])\n",
    "print([token.surface for token in a.analyze(text) if token.part_of_speech.split(',')[0] in ['動詞', '名詞']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】ニュースの分析\n",
    "\n",
    "目的\n",
    "\n",
    "- 日本語の自然言語処理の体験\n",
    "- 類似度の理解\n",
    "\n",
    "以下からldcc-20140209.tar.gzをダウンロードしてください。   \n",
    "[livedoor](https://www.rondhuit.com/download.html#ldcc)\n",
    "\n",
    "もしくはwgetコマンドを使っても良いです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# livedoorのnewsをダウンロード\n",
    "wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
    "# 圧縮ファイルを解凍\n",
    "tar zxf ldcc-20140209.tar.gz\n",
    "# livedoorニュースの説明を表示\n",
    "cat text/README.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodingをutf-8指定して読み込み\n",
    "bin_data = load_files('./text', encoding='utf-8')\n",
    "documents = bin_data.data\n",
    "# 今回はラベルが無いと仮定してください\n",
    "# targets = bin_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問】\n",
    "\n",
    "以下の流れでニュースを分析してください。\n",
    "\n",
    "- まずどんなニュースなのか読んでみる\n",
    "- 出現単語をカウントして分析する\n",
    "- テキストをクリーニングする\n",
    "- BoW + TFIDFでベクトル化する\n",
    "- あるニュースに一番cos類似度が近いニュースを出力する関数の作成\n",
    "- 別の類似度手法を1つ調べて上の関数に組み込む(切り替えられるようにする)\n",
    "- なぜそのような結果になったのか考察する\n",
    "\n",
    "[sklearn.feature_extraction.text](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://news.livedoor.com/article/detail/4931238/\\n2010-08-08T10:00:00+0900\\nNY名物イベントが日本でも！名店グルメを気軽に楽しむ\\nニューヨークで20年続いている食の祭典「レストラン・ウィーク」。その日本版がダイナーズクラブ特別協賛のもと7月30日よりスタート。8月31日までの期間中、青山・六本木、丸の内、銀座、横浜のエリアから、ラグジュアリーレストラン81店舗がこのイベントのために特別用意したランチメニュー2010円（税・サ別）、ディナー5000円（税・サ別）を気軽に楽しめる、とっておきのイベントです。\\n\\u3000\\n\\u3000実行委員長には、学校法人服部学園、服部栄養専門学校 理事長・校長であり医学博士でもある服部幸應氏を迎え、実行委員に石田純一さん、LA BETTOLAオーナーシェフ落合務氏、フードアナリスト協会会長、高賀右近氏、つきぢ田村三代目、田村隆氏に、そして放送作家・脚本家の小山薫堂さんなど、食のスペシャリストたちが勢揃い。\\n\\n参加レストランには、ミシュランのフランス版、東京版ともに星を獲得している吉野建シェフの「レストラン タテル ヨシノ 汐留」や、日本料理の名門「つきぢ田村」、「金田中 庵」、「赤坂璃宮」に「mikuni MARUNOUCHI」など、日本を代表するレストランがずらり。\\n\\u3000イベント期間の〜8月19日までは、特別協賛のダイナーズクラブカード会員、またはシティバンクに口座を持つシティゴールドメンバーが楽しめる先行期間となりますが、その後は誰でも参加できるので、日程のチェックは必須。\\n\\n\\u3000予約方法は必ず事前に、各店舗に問合せを行い「ジャパンレストラン・ウィーク2010」での予約であることを伝えればOK！憧れていたレストランの料理をリーズナブルにいただけるチャンスです！極上の味とラグジュアリーな空間を満喫。そんな幸せを実感できる「ジャパンレストラン・ウィーク2010」にぜひ参加しててみてはいかがですか？\\n\\nJAPAN RESTAURANT WEEK 2010 -公式サイト\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#最初のニュースの内容\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('http', 1), ('://', 1), ('news', 1), ('.', 2), ('livedoor', 1), ('com', 1), ('/', 4), ('article', 1), ('detail', 1), ('4931238', 1), ('\\n', 6), ('2010', 5), ('-', 3), ('08', 2), ('T', 1), ('10', 1), (':', 2), ('00', 2), ('+', 1), ('0900', 1), ('NY', 1), ('名物', 1), ('イベント', 4), ('が', 7), ('日本', 4), ('で', 6), ('も', 2), ('！', 3), ('名店', 1), ('グルメ', 1), ('を', 11), ('気軽', 2), ('に', 14), ('楽しむ', 1), ('ニューヨーク', 1), ('20', 1), ('年', 1), ('続い', 1), ('て', 7), ('いる', 2), ('食', 2), ('の', 17), ('祭典', 1), ('「', 8), ('レストラン', 7), ('・', 8), ('ウィーク', 3), ('」', 8), ('。', 7), ('その', 1), ('版', 3), ('ダイナーズクラブ', 1), ('特別', 3), ('協賛', 2), ('もと', 1), ('7', 1), ('月', 3), ('30', 1), ('日', 3), ('より', 1), ('スタート', 1), ('8', 2), ('31', 1), ('まで', 2), ('期間', 3), ('中', 1), ('、', 28), ('青山', 1), ('六本木', 1), ('丸の内', 1), ('銀座', 1), ('横浜', 1), ('エリア', 1), ('から', 1), ('ラグジュアリーレストラン', 1), ('81', 1), ('店舗', 2), ('この', 1), ('ため', 1), ('用意', 1), ('し', 3), ('た', 2), ('ランチ', 1), ('メニュー', 1), ('円', 2), ('（', 2), ('税', 2), ('サ', 2), ('別', 2), ('）', 2), ('ディナー', 1), ('5000', 1), ('楽しめる', 2), ('とっ', 1), ('おき', 1), ('です', 3), ('\\u3000', 4), ('実行', 2), ('委員', 2), ('長', 2), ('は', 7), ('学校', 2), ('法人', 1), ('服部', 3), ('学園', 1), ('栄養', 1), ('専門', 1), (' ', 11), ('理事', 1), ('校長', 1), ('あり', 1), ('医学', 1), ('博士', 1), ('ある', 2), ('幸', 1), ('應氏', 1), ('迎え', 1), ('石田', 1), ('純一', 1), ('さん', 2), ('LA', 1), ('BETTOLA', 1), ('オーナー', 1), ('シェフ', 2), ('落合', 1), ('務', 1), ('氏', 3), ('フード', 1), ('アナリスト', 1), ('協会', 1), ('会長', 1), ('高賀', 1), ('右近', 1), ('つき', 2), ('ぢ', 2), ('田村', 3), ('三', 1), ('代目', 1), ('隆', 1), ('そして', 1), ('放送', 1), ('作家', 1), ('脚本', 1), ('家', 1), ('小山', 1), ('薫', 1), ('堂', 1), ('など', 2), ('スペシャリスト', 1), ('たち', 1), ('勢揃い', 1), ('\\n\\n', 3), ('参加', 3), ('ミシュラン', 1), ('フランス', 1), ('東京', 1), ('とも', 1), ('星', 1), ('獲得', 1), ('吉野', 1), ('建', 1), ('タテル', 1), ('ヨシノ', 1), ('汐留', 1), ('や', 1), ('料理', 2), ('名門', 1), ('金', 1), ('田中', 1), ('庵', 1), ('赤坂', 1), ('璃宮', 1), ('mikuni', 1), ('MARUNOUCHI', 1), ('代表', 1), ('する', 1), ('ずらり', 1), ('〜', 1), ('19', 1), ('ダイナーズクラブカード', 1), ('会員', 1), ('または', 1), ('シティバンク', 1), ('口座', 1), ('持つ', 1), ('シティゴールドメンバー', 1), ('先行', 1), ('と', 2), ('なり', 1), ('ます', 1), ('その後', 1), ('誰', 1), ('でも', 1), ('できる', 2), ('ので', 1), ('日程', 1), ('チェック', 1), ('必須', 1), ('予約', 2), ('方法', 1), ('必ず', 1), ('事前', 1), ('各', 1), ('問合せ', 1), ('行い', 1), ('ジャパン', 2), ('こと', 1), ('伝えれ', 1), ('ば', 1), ('OK', 1), ('憧れ', 1), ('い', 1), ('リーズナブル', 1), ('いただける', 1), ('チャンス', 1), ('極上', 1), ('味', 1), ('ラグジュアリー', 1), ('な', 1), ('空間', 1), ('満喫', 1), ('そんな', 1), ('幸せ', 1), ('実感', 1), ('ぜひ', 1), ('み', 1), ('いかが', 1), ('か', 1), ('？', 1), ('JAPAN', 1), ('RESTAURANT', 1), ('WEEK', 1), ('公式', 1), ('サイト', 1)]\n"
     ]
    }
   ],
   "source": [
    "#最初の文章の出現単語\n",
    "a = Analyzer(token_filters=[TokenCountFilter()])\n",
    "\n",
    "g_count = list(a.analyze(documents[0]))\n",
    "\n",
    "print(g_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<>\n",
    "kakko1 = '<.*?>'\n",
    "\n",
    "#【】\n",
    "kakko2 = '【.*?】'\n",
    "\n",
    "#特殊文字\n",
    "tokusyu = '[\\n\\*`\\s■◆○★●]'\n",
    "\n",
    "#URL\n",
    "url = '(http.*?\\n)'\n",
    "\n",
    "#日時\n",
    "date = '(\\d{4}-.*?\\+\\d{4})'\n",
    "\n",
    "#連結\n",
    "reg_str = f'{kakko1}|{kakko2}|{tokusyu}|{url}|{date}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NY名物イベントが日本でも！名店グルメを気軽に楽しむニューヨークで20年続いている食の祭典「レストラン・ウィーク」。その日本版がダイナーズクラブ特別協賛のもと7月30日よりスタート。8月31日までの期間中、青山・六本木、丸の内、銀座、横浜のエリアから、ラグジュアリーレストラン81店舗がこのイベントのために特別用意したランチメニュー2010円（税・サ別）、ディナー5000円（税・サ別）を気軽に楽しめる、とっておきのイベントです。実行委員長には、学校法人服部学園、服部栄養専門学校理事長・校長であり医学博士でもある服部幸應氏を迎え、実行委員に石田純一さん、LABETTOLAオーナーシェフ落合務氏、フードアナリスト協会会長、高賀右近氏、つきぢ田村三代目、田村隆氏に、そして放送作家・脚本家の小山薫堂さんなど、食のスペシャリストたちが勢揃い。参加レストランには、ミシュランのフランス版、東京版ともに星を獲得している吉野建シェフの「レストランタテルヨシノ汐留」や、日本料理の名門「つきぢ田村」、「金田中庵」、「赤坂璃宮」に「mikuniMARUNOUCHI」など、日本を代表するレストランがずらり。イベント期間の〜8月19日までは、特別協賛のダイナーズクラブカード会員、またはシティバンクに口座を持つシティゴールドメンバーが楽しめる先行期間となりますが、その後は誰でも参加できるので、日程のチェックは必須。予約方法は必ず事前に、各店舗に問合せを行い「ジャパンレストラン・ウィーク2010」での予約であることを伝えればOK！憧れていたレストランの料理をリーズナブルにいただけるチャンスです！極上の味とラグジュアリーな空間を満喫。そんな幸せを実感できる「ジャパンレストラン・ウィーク2010」にぜひ参加しててみてはいかがですか？JAPANRESTAURANTWEEK2010-公式サイト\n"
     ]
    }
   ],
   "source": [
    "#テキストクリーニングしてリストに戻す\n",
    "document = []\n",
    "\n",
    "for i in documents:\n",
    "    remove = re.compile(reg_str)\n",
    "    document.append(re.sub(remove, '', i))\n",
    "\n",
    "#最初のニュース\n",
    "print(document[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7b9fe13ad5491c953991f3395fbe6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7376), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#形態素解析してリストに戻す(複合名詞　+ 動詞)\n",
    "corpus = []\n",
    "\n",
    "a = Analyzer(token_filters=[CompoundNounFilter()])\n",
    "for text in tqdm_notebook(document):\n",
    "    corpus.append(' '.join([token.surface for token in a.analyze(text) \n",
    "                                    if token.part_of_speech.split(',')[0] in ['動詞', '名詞']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NY名物イベント 日本 名店グルメ 気軽 楽しむ ニューヨーク 20年 続い いる 食 祭典 レストラン ウィーク 日本版 ダイナーズクラブ特別協賛 もと7月30日 スタート 8月31日 期間中 青山 六本木 丸の内 銀座 横浜 エリア ラグジュアリーレストラン81店舗 イベント ため 特別用意 し ランチメニュー2010円 税 サ別 ディナー5000円 税 サ別 気軽 楽しめる とっ おき イベント 実行委員長 学校法人服部学園 服部栄養専門学校理事長 校長 医学博士 服部幸應氏 迎え 実行委員 石田純一さん LABETTOLAオーナーシェフ落合務氏 フードアナリスト協会会長 高賀右近氏 つき ぢ田村三代目 田村隆氏 放送作家 脚本家 小山薫堂さん 食 スペシャリストたち 勢揃い 参加レストラン ミシュラン フランス版 東京版とも 星 獲得 し いる 吉野建シェフ レストランタテルヨシノ汐留 日本料理 名門 つき ぢ田村 金田中庵 赤坂璃宮 mikuniMARUNOUCHI 日本 代表 する レストラン イベント期間 8月19日 特別協賛 ダイナーズクラブカード会員 シティバンク 口座 持つ シティゴールドメンバー 楽しめる 先行期間 なり その後 誰 参加 できる 日程 チェック 必須 予約方法 事前 店舗 問合せ 行い ジャパンレストラン ウィーク2010 予約 こと 伝えれ OK 憧れ い レストラン 料理 リーズナブル いただける チャンス 極上 味 ラグジュアリー 空間 満喫 幸せ 実感 できる ジャパンレストラン ウィーク2010 参加 し て み いかが JAPANRESTAURANTWEEK2010-公式サイト\n"
     ]
    }
   ],
   "source": [
    "#形態素解析後のニュース\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#BoW\n",
    "count = CountVectorizer()\n",
    "\n",
    "docs = np.array(corpus)\n",
    "bag = count.fit_transform(docs)\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "tfidf = TfidfTransformer()\n",
    "vector = tfidf.fit_transform(bag).toarray()\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(documents, news_num=1, sim='cos'):\n",
    "    '''\n",
    "    cos類似度を求める\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    documents : リスト\n",
    "        様々なニュース\n",
    "    news_num : int\n",
    "        何番目のニュースか(デフォルト : 1(1番目のニュース))\n",
    "    sim : str\n",
    "        類似度(デフォルト : cos(cos類似度))\n",
    "    ''' \n",
    "    #様々なニュース\n",
    "    #形態素解析してリストに戻す(複合名詞　+ 動詞)\n",
    "    corpus = []\n",
    "    a = Analyzer(token_filters=[CompoundNounFilter()])\n",
    "    for document in tqdm_notebook(documents):\n",
    "        corpus.append(' '.join([token.surface for token in a.analyze(document) \n",
    "                                        if token.part_of_speech.split(',')[0] in ['動詞', '名詞']]))\n",
    "    \n",
    "    #Bow\n",
    "    count = CountVectorizer()\n",
    "    docs = np.array(corpus)\n",
    "    bag = count.fit_transform(docs)\n",
    "    \n",
    "    #TF-IDF\n",
    "    fidf = TfidfTransformer()\n",
    "    vector = tfidf.fit_transform(bag).toarray()\n",
    "    \n",
    "    #ターゲットのニュース\n",
    "    news_num -= 1 #ニュースの番号\n",
    "    news = docs[news_num] #ニュースの内容\n",
    "    news_corpus = corpus[news_num] #形態素解析したニュース\n",
    "    news_vector = vector[news_num] #TF-IDFしたニュース\n",
    "    \n",
    "\n",
    "    #cos類似度の場合\n",
    "    if sim == 'cos':\n",
    "        \n",
    "        #初期値\n",
    "        cos_max = 0 #最大のcos類似度\n",
    "        index = 0 #cos類似度が最大の時のindex\n",
    "\n",
    "        #cos類似度を計算\n",
    "        for i, doc_vec in enumerate(vector):        \n",
    "            cos = np.dot(news_vector, doc_vec) / (np.linalg.norm(news_vector) * np.linalg.norm(doc_vec))\n",
    "\n",
    "            #同じ文章ならpass\n",
    "            if int(cos) == 1:\n",
    "                pass\n",
    "\n",
    "            #cos類似度が最高ならcos類似度とインデックスを更新\n",
    "            else:\n",
    "                if cos_max < cos:\n",
    "                    cos_max = cos\n",
    "                    index = i\n",
    "\n",
    "        print('cos類似度　:　{:.3}'.format(cos_max) + '\\n' + documents[index])\n",
    "        \n",
    "        \n",
    "    #doc2vec\n",
    "    elif sim == 'doc2vec':\n",
    "        \n",
    "        #初期値\n",
    "        training_docs = [] \n",
    "\n",
    "        #ターゲットのニュースをリストに入れる\n",
    "        sent1 = TaggedDocument(words=news_corpus.split(' '), tags=[news])\n",
    "        training_docs.append(sent1)\n",
    "        \n",
    "        #全てのニュースをリストに入れる\n",
    "        for i, (doc_corpus, doc) in enumerate(zip(corpus, documents)):\n",
    "            \n",
    "            #ターゲットのニュースと同じニュースはpass、それ以外はリストへ\n",
    "            if i == news_num:\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                sent2 = TaggedDocument(words=doc_corpus.split(' '), tags=[doc])\n",
    "                training_docs.append(sent2)\n",
    "\n",
    "        #類似度を求め、最も高いものを出力\n",
    "        model = Doc2Vec(documents=training_docs)\n",
    "        similar = model.docvecs.most_similar(news, topn=1)\n",
    "        \n",
    "        print(similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cos類似度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7376), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cos類似度　:　0.0727\n",
      "一度参加すればハマる？噂の“街コン”に参加してみた！昨年あたりから日本各地で流行している街ぐるみの合コンイベント“街コン”。有名なのは栃木の宇都宮や広島、福島といったところで、男女合わせて2000人以上が参加しているらしい。なぜこんなに多くの人が参加するのか？興味はあったが、筆者は首都圏在住なので参加するにはちょっと遠いのだ。しかしここ最近は東京をはじめとする首都圏でも、数多くの街コンが開催されているという。1000人規模とはいかなくても、200人とか400人とか、どこもそれなりに規模は大きい様子。これはぜひ参加して婚活せねば！ということで早速某駅周辺のイベントに参加してきたのだが……いやーすごかった。まず参加してビックリしたのは、皆のやる気。受付開始からイベントがスタートするまで約2時間あったのだが、余裕を持って1時間前に到着したらもう大行列！聞くと受付開始30分過ぎにはもうこんな状態だったという。あらら〜と後悔しても仕方がないので大人しく並び受付を済ませると、リストバンドと地図が配布される。リストバンドは参加者の目印で、これがあれば街でイベントに参加しているいくつかの店舗に自由に出入りすることができ、飲み食いし放題！そしてその店の情報が掲載されているのが地図。これをみながら探索するという仕組みだ。さっそく店に入って席につくと、案内の人が参加者の男性と同席できるようセッティングしてくれた。そこでまずご飯を食べながら交流がスタート。「どこから来ました？」「年齢は？」「社会人ですか？」といった自己紹介がされ、あとはお互いを探り探りトークへ突入する。ここまでは普通の合コンと大差ないが、大きく違うのは“気に入らなかったら他にいける”ということ。そういう場合は「他の店にいってみたいので〜」などと適当に理由をつけて店を出ちゃえばいいのだ。で、他の店に行けばまた違う男性と同席が可能。つまり、店ごとに違う合コンを何度もやっている感覚である。これは確かに新しい。さらに男性との交流以外にも料理を食べたり飲んだりできるという楽しさもあるし、男性以外にも女性の友達を作るのもアリだ。実際筆者も一番楽しかったのは、バーカウンターで一緒になった同年代の女性たちとのトーク。皆男性をそっちのけで「いやー、いい出会いってないよねー」といった婚活話に花を咲かせる始末である。こんなんだから結婚ができないのかもしれないが……。しかし参加してみて思ったのは、皆結局「出会いが欲しい」というホンネ。だからここまで全国的に盛り上がっているのだと思う。とはいえそんなホンネをダイレクトにぶつけるのは、やっぱり嫌なのだ。でも街コンなら参加する意味としても「いろいろな店で飲んだり食べたりするのが楽しい」とか「女の子の知り合いもつくれる」といった、恋愛以外の大義名分がすごく立てやすい。だから参加するほうも気軽だし、楽しいように感じる。結局筆者は街コン参加後、トークが盛り上がった女性たちを含む男女8人くらいでカラオケ店へ。最後は「またみんなで会おうね〜」という軽いノリで別れた。こういった出会いは利害関係も後腐れも一切ないので純粋に楽しいが、そこから恋愛に発展させるとなると、もうひと頑張り必要なのも事実か。そこを突破できるかどうかなんだろうな……などと思いながらも、結局何もしていないダメっぷりである。ちなみに一応取材目線として感じたことも。参加した街は大きな大学がある学生街だったにもかかわらず、そういった学生ノリの人はごく少数。ほとんどは30歳間近の社会人が中心だった。それだけ出会いに飢えているのは社会人なのかもしれない。また今回私が参加したイベントでは遭遇しなかったが、最近では女性の体目的で街コンを荒らす男性たちも増えているという話も耳にした。そういったことには注意したほうがいいかもしれない。ただ、だからといって警戒心を強く持つと楽しめない。大人の良識を持ちつつ気楽に楽しむのがいいように感じた。（橋口まどか）\n"
     ]
    }
   ],
   "source": [
    "#1番最初のニュースと似ているニュースを探す\n",
    "similar(document, news_num=1, sim='cos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**doc2vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a8f88a8e664823a886b93bb92fa782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7376), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[('「デルレイ」のコラボショコラデザートコースで、今年は特別なバレンタインクリスマス、忘年会、新年会がひと段落したと思ったら、気付けばバレンタインデーまであと1ヶ月あまり。大切なあの人への贈り物はとびっきり特別にしたいのに、なんとなく毎年同じようなショコラを選んでしまう…。そんな想いをした経験がある女子も多いのでは無いでしょうか？そんな方に朗報です！“しあわせのリンク＆スマイル”をテーマにいつも最上級のくつろぎを提案するホテル日航東京が、高級老舗ショコラティエ「デルレイ」とコラボレーションした世界初の「ショコラデザートコース」を提供。コース仕立てで楽しむチョコレートの味わいが、2人の時間をさらに幸せに演出します。デザートをコースで楽しむなんて、とても贅沢！気になる中身は、濃厚なカカオが香るあたたかいチョコレートドリンク「ショコラ・ショー」、ほどよい酸味が心地よい季節のフルーツ「あまおう苺」を使用した、ホテルオリジナルの「あまおう苺とシャンパンジュレ」とバラエティに富んだスイーツが次々に登場。3品目には、「デルレイ」の得意技である、「フルーツフランベ」。シャンパンとフレッシュフルーツを加え仕上げをしてくれるので、香りにうっとりと酔いしれることができます。そして、「デルレイ」のアイコンとも言える燦然と輝くダイヤモンド、デルレイ監修によるホテルオリジナルのショコラマカロン、ガトーショコラの「プレミアムショコラプレート」で大満足のラスト。このコースが味わえるラウンジ＆シャンパンバー「ベランダ」は、レインボーブリッジが見えるキラッキラの夜景が魅力的。常時置いてある、14種類のシャンパンの他、「ショコラデザートコース」にぴったりな特別銘柄が期間限定で登場。シャンパンと味わう新しいショコラの楽しみ方で、今年のバレンタインデーはちょっぴり差をつけてみませんか？「ショコラデザートコース」期間：2010年2月1日（月）〜2月14日（日）レストラン：ラウンジ＆シャンパンバー「ベランダ」（ティータイム10：00〜17：00／バータイム17：00〜23：00）コース内容：ショコラデザートコース3,150円※消費税込み、サービス料別・あまおう苺とシャンパンジュレシャンパンケーキを添えて・デルレイショコラ・ショーとプティ・フール・セック・シャンパンフルーツシュゼット・プレミアムショコラプレート・コーヒー又は紅茶ラウンジ＆シャンパンバー「ベランダ」03-5500-570310：00〜23：00（2/1〜2/2821：30まで。3/1より営業時間が10：00〜21：30と変更になります）☆才職兼美バレンタイン特集2010', 0.9477887153625488)]\n"
     ]
    }
   ],
   "source": [
    "#1番最初のニュースと似ているニュースを探す(文章、類似度の順で出力)\n",
    "similar(document, news_num=1, sim='doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1番最初のニュースは、レストランのイベント開催に関するニュースであり、これと似ているニュースを検索した。\n",
    "cos類似度の場合、街コンに関するニュースが出力され、このときの類似度は0.0727であった。これはイベントという部分は同じだが、cos類似度が0.0727と低いため、あまり似ているとは言えない。\n",
    "\n",
    "doc2vecの場合、ホテルのレストランでのバレンタインイベントに関するニュースが出力され、この時の類似度0.9478はであった。こちらはどちらもレストランのイベント開催のお知らせという共通点があり、類似度も0.9478と非常に高かった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】感情分析\n",
    "\n",
    "目的\n",
    "\n",
    "- NLP定番の感情分析の経験\n",
    "- 英語の処理の実践\n",
    "\n",
    "以下からLarge Movie Review Datasetをダウンロードしてください。\n",
    "\n",
    "[IMDBレビュー](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "\n",
    "同じようにwgetコマンドでも可能です。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# IMDBをカレントフォルダにダウンロード\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# 解凍\n",
    "!tar zxf aclImdb_v1.tar.gz\n",
    "# aclImdb/train/unsupはラベル無しのため削除\n",
    "!rm -rf aclImdb/train/unsup\n",
    "# IMDBデータセットの説明を表示\n",
    "!cat aclImdb/README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの読み込み\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "train_x, train_y = train_review.data, train_review.target\n",
    "\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "test_x, test_y = test_review.data, test_review.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問】\n",
    "\n",
    "IMDBという映画に対するレビューのデータセットを使います。  \n",
    "良いレビューか悪いレビューかを判定するモデルを作ってください。  \n",
    "テストデータに対する正解率が90%を超えるまで、調査=>実行=>改善を繰り返してください。  \n",
    "前処理になぜその処理をしたのかを書くとエンジニアリングとしても完璧です。\n",
    "\n",
    "注意: 必ず間違っていたデータを観察してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#1番最初の学習用データ\n",
    "print(train_x[0])\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#学習用とテスト用のデータをくっつける\n",
    "train_x[len(train_x): len(train_x)] = test_x\n",
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<br>\n",
    "br = '<.*?>'\n",
    "\n",
    "#特殊文字\n",
    "tokusyu = \"[\\?!\\/,.'\\(\\):\\\"-;]\"\n",
    "\n",
    "#連結\n",
    "reg_str = f'{br}|{tokusyu}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero day leads you to think  even re think why two boys young men would do what they did   commit mutual suicide via slaughtering their classmates  it captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own mutual world via coupled destruction   it is not a perfect movie but given what money time the filmmaker and actors had   it is a remarkable product  in terms of explaining the motives and actions of the two young suicide murderers it is better than  elephant    in terms of being a film that gets under our  rationalistic  skin it is a far  far better film than almost anything you are likely to see    flawed but honest with a terrible honesty \n"
     ]
    }
   ],
   "source": [
    "#テキストクリーニングしてリストに戻す\n",
    "text = []\n",
    "\n",
    "for i in train_x:\n",
    "    #ぜんぶ小文字に\n",
    "    i = i.lower()\n",
    "    \n",
    "    #正規表現で削除\n",
    "    remove = re.compile(reg_str)\n",
    "    text.append(re.sub(remove, ' ', i))\n",
    "\n",
    "#最初のニュース\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習用とテスト用に戻す\n",
    "X_train = text[:25000]\n",
    "X_test = text[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM_RANGE = (1, 2)\n",
    "TOP_K = 10000\n",
    "TOKEN_MODE = 'word'\n",
    "MIN_DOC_FREQ = 2\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, test_texts):\n",
    "    kwargs = {\n",
    "        'ngram_range' : NGRAM_RANGE,\n",
    "        'dtype' : 'int32',\n",
    "        'strip_accents' : 'unicode',\n",
    "        'decode_error' : 'replace',\n",
    "        'analyzer' : TOKEN_MODE,\n",
    "        'min_df' : MIN_DOC_FREQ,\n",
    "    }\n",
    "    \n",
    "    # Learn Vocab from train texts and vectorize train and val sets\n",
    "    tfidf_vectorizer = TfidfVectorizer(**kwargs)\n",
    "    x_train = tfidf_vectorizer.fit_transform(train_texts)\n",
    "    x_test = tfidf_vectorizer.transform(test_texts)\n",
    "    \n",
    "    # Select best k features, with feature importance measured by f_classif\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_test = selector.transform(x_test).astype('float32')\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuhisoejima/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1577: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = ngram_vectorize(X_train, train_y, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_4 (Dropout)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                160016    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,305\n",
      "Trainable params: 160,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 入力の形式は映画レビューで使われている語彙数\n",
    "vocab_size = X_train.shape[1:]\n",
    "\n",
    "#モデル\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.5, input_shape=vocab_size, ))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#コンパイル\n",
    "model.compile(optimizer=Adam(lr=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#コールバック\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 3s 115us/step - loss: 0.6410 - acc: 0.6543 - val_loss: 0.4963 - val_acc: 0.8695\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 3s 109us/step - loss: 0.4296 - acc: 0.8413 - val_loss: 0.3070 - val_acc: 0.8958\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 3s 106us/step - loss: 0.3326 - acc: 0.8790 - val_loss: 0.2660 - val_acc: 0.9017\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 3s 102us/step - loss: 0.3051 - acc: 0.8897 - val_loss: 0.2513 - val_acc: 0.9049\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 3s 101us/step - loss: 0.2862 - acc: 0.8975 - val_loss: 0.2474 - val_acc: 0.9041\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 3s 100us/step - loss: 0.2718 - acc: 0.9017 - val_loss: 0.2471 - val_acc: 0.9019\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 3s 100us/step - loss: 0.2615 - acc: 0.9063 - val_loss: 0.2447 - val_acc: 0.9031\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 2s 99us/step - loss: 0.2493 - acc: 0.9121 - val_loss: 0.2432 - val_acc: 0.9034\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 3s 101us/step - loss: 0.2434 - acc: 0.9135 - val_loss: 0.2468 - val_acc: 0.9027\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 2s 96us/step - loss: 0.2342 - acc: 0.9164 - val_loss: 0.2437 - val_acc: 0.9039\n"
     ]
    }
   ],
   "source": [
    "#学習\n",
    "history = model.fit(X_train,\n",
    "                    train_y,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_test, test_y), \n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.24373817365646364\n",
      "acc : 0.903919999961853\n"
     ]
    }
   ],
   "source": [
    "#結果\n",
    "history = history.history\n",
    "test_acc = history['val_acc'][-1]\n",
    "test_loss = history['val_loss'][-1]\n",
    "\n",
    "print('loss :', test_loss)\n",
    "print('acc :', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 11, 32, 37, 43])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#間違ったデータの確認\n",
    "np.where((np.where(model.predict(X_test) < 0.5, 0, 1).reshape(-1) == test_y) == False)[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いい評価を悪い評価と推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't hate Heather Graham because she's beautiful, hate her because she's fun to watch in this movie. Like the hip clothing and funky surroundings, the actors in this flick work well together. Casey Affleck is hysterical and Heather Graham literally lights up the screen. The minor characters - Goran Visnjic {sigh} and Patricia Velazquez are as TALENTED as they are gorgeous. Congratulations Miramax & Director Lisa Krueger!\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(test_x[0])\n",
    "print(test_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hateやfunなど、negativeな言葉とpositiveな言葉が入り混じっていた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "悪い評価をいい評価と推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would not recommend this movie. Even though it is rated G and is clearly for kids there is quite a lot of swearing (including the dreaded 'F' and 'S' words). This kind of language doesn't offend me particularly but in a kids film? Come on.<br /><br />There was also quite a bit of implied sexual content, between one of the early adolescent male characters and any willing adult woman who came along - including a prostitute! <br /><br />The acting was as good as it gets in this genre of film but the story line was very very cheesy and even my four year old remarked that it was 'stupid'.<br /><br />Despite having Elizabeth Shue, this film is definitely not worth checking out if you haven't seen it.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_x[32])\n",
    "print(test_y[32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これもgoodやstupidなど、negativeな言葉とpositiveな言葉が入り混じっていたが、形容詞をあまり使わず批判を行なっている文章であると感じた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題7】分散表現(アドバンス課題)\n",
    "\n",
    "目的\n",
    "\n",
    "- 主流である分散表現の理解\n",
    "\n",
    "[分散表現の詳細はこちらを参考にしてください]()\n",
    "\n",
    "【問】\n",
    "以下の中から一つ選んで分散表現を獲得し、  \n",
    "好きな単語群をt-SNE,PCAなどを用いて可視化してください。  \n",
    "コーパスは自由です。\n",
    "\n",
    "- Word2Vec-CBoW(2〜3人)\n",
    "- Word2Vec-skip-gram(2〜3人)\n",
    "- fastText(2〜3人)\n",
    "\n",
    "また以下の4点についてもノートに書いてください。\n",
    "\n",
    "- 分布仮説とは何か？\n",
    "- 分散表現を得ることのメリットは何か？\n",
    "- 上で選んだモデルのメリット、デメリットは何か？\n",
    "- なぜそのパラメータを選んだのか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】自然言語処理の応用事例\n",
    "\n",
    "目的\n",
    "\n",
    "- NLPの情報共有\n",
    "現在自然言語処理はどのような企業でどのように活用されているか？   \n",
    "1つ例をあげて3~5分で発表してください。   \n",
    "(例)メルカリは商品説明をTF-IDFを用いてベクトル化して商品の異常検知を行っている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gunosyでは、自然言語処理を用いて記事を政治やスポーツ等に分類しており、各ユーザーに適している記事を配信している。\n",
    "\n",
    "[Gunosy における AWS 上での自然言語処理・機械学習の活用事例](https://www.slideshare.net/keisukeosone/gunosy-aws-76649598)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深く学びたい方へ\n",
    "\n",
    "- [首都大学小町研/推薦書籍](http://cl.sd.tmu.ac.jp/prospective/readings)\n",
    "- [NAIST勉強会](http://www.phontron.com/teaching.php?lang=ja)\n",
    "- [NLP100本ノック](http://www.cl.ecei.tohoku.ac.jp/nlp100/)\n",
    "- [ACL](https://acl2018.org/)\n",
    "- [NMNLP](https://emnlp2018.org/)\n",
    "- [おすすめNLPデータセット](https://gengo.ai/ja/datasets/the-best-25-datasets-for-natural-language-processing/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
